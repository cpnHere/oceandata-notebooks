{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91616035",
   "metadata": {},
   "source": [
    "# Title of the Tutorial\n",
    "\n",
    "**Authors:** Ian Carroll (NASA, UMBC)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "\n",
    "The following notebooks are **prerequisites** for this tutorial.\n",
    "\n",
    "- Learn with OCI: [Data Access][oci-data-access]\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "An [Earthdata Login][edl] account is required to access data from the NASA Earthdata system, including NASA ocean color data.\n",
    "\n",
    "</div>\n",
    "\n",
    "[edl]: https://urs.earthdata.nasa.gov/\n",
    "[oci-data-access]: https://oceancolor.gsfc.nasa.gov/resources/docs/tutorials/notebooks/oci_data_access/\n",
    "\n",
    "## Summary\n",
    "\n",
    "Parallel and Out-of-core Processing\n",
    "\n",
    "ideas\n",
    "- looking up points, potentially for matchups\n",
    "- gridding L2 granules\n",
    "- flow #1\n",
    "  - read a seabass file with point information\n",
    "  - search for granules that contain points\n",
    "  - join pixel values with seabass data\n",
    "- flow #2\n",
    "  - get granules\n",
    "  - grid/interpolate them\n",
    "    - aggregation with rolling or groupby?\n",
    "    - do it with lots of granules?\n",
    "\n",
    "- questions\n",
    "  - what does cartopy do, when projecting, given swath data with transform=PlateCaree?\n",
    "  - and how does that differ from pyresample\n",
    "  - is a dask array cached? how can you tell?\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "At the end of this notebook you will know:\n",
    "\n",
    "- How to ...\n",
    "- What ...\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "1. [Setup](#setup)\n",
    "\n",
    "<a name=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea144446",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "**Compiled Functions**\n",
    "\n",
    "doing things in loops, and doing things with numba just-in-time compiled functions in loops, vs arrays\n",
    "\n",
    "introduces notion of compiled vs interpreted evaluation, and reason for doing everything in arrays (no need for numba)\n",
    "\n",
    "**Task Graph**\n",
    "\n",
    "Consider introducing this notion first, and using it to expalain parallel and larger-than memory in the same language.\n",
    "\n",
    "\n",
    "\n",
    "**Parallel**\n",
    "\n",
    "parallel can (and usually does in data processing) mean evaluating the same code on different inputs. our loop above (but not all loops) are a good example of \"evaluating the same code on different inputs\".\n",
    "\n",
    "parallel will obviously speed things up but you have to have the resources to do it\n",
    "- number of tasks\n",
    "- memory per task\n",
    "\n",
    "intrinsically this introduces the notion of a \"queue\" if you introduce more tasks than your system can run simultaneously, due to resource limitations\n",
    "\n",
    "just a long running computation, or maybe looping through some computation on granules without chunks\n",
    "\n",
    "could use as an opportunity to set up resampling without dask\n",
    "\n",
    "parallelize with dask bag, or njit? what's the problem here? you still have to manage memory carefully and not ask for more threads than your system can handle\n",
    "\n",
    "**Memory**\n",
    "\n",
    "parallel with queuing allows larger-than-memory computation\n",
    "\n",
    "numpy random\n",
    "\n",
    "dask.array random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c037c",
   "metadata": {},
   "source": [
    "**get out of the way now**\n",
    "- `%%time` and `%%timeit`\n",
    "- `logging`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0a197",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Begin by importing all of the packages used in this notebook. If your kernel uses an environment defined following the guidance on the [tutorials] page, then the imports will be successful.\n",
    "\n",
    "[tutorials]: https://oceancolor.gsfc.nasa.gov/resources/docs/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc236c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from dask.distributed import Client\n",
    "from numba import njit\n",
    "import dask.array as da\n",
    "import earthaccess\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e55c5",
   "metadata": {},
   "source": [
    "Turn on informational alerts from `earthaccess`, because we are still learning what it's\n",
    "doing. There is no need for the cell below, or `import logging` above, if you do not want\n",
    "these alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger(\"earthaccess\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b43586",
   "metadata": {},
   "source": [
    "Use a fixed but unique seed, such as your social security number or `secrets.randbits(64)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf023f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = np.random.default_rng(seed=5179916885778238210)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f466c10",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    \n",
    "The `persist=True` argument ensures any discovered credentials are\n",
    "stored in a `.netrc` file, so the argument is not necessary (but\n",
    "it's also harmless) for subsequent calls to `earthaccess.login`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a37321",
   "metadata": {},
   "outputs": [],
   "source": [
    "tspan = (\"2024-06\", \"2024-06\")\n",
    "bbox = (-76.75, 36.97, -75.74, 39.01)\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"PACE_OCI_L2_AOP_NRT\",\n",
    "    temporal=tspan,\n",
    "    bounding_box=bbox,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cffd27",
   "metadata": {},
   "source": [
    "[back to top](#contents) <a name=\"section-name\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01565ffe",
   "metadata": {},
   "source": [
    "## x. Compiled Functions\n",
    "\n",
    "When you are interested in performance improvements for data processing, the first tool\n",
    "in your kit is compiled functions. If you use NumPy, you have already checked this box.\n",
    "However, since you may sometimes need `numba`, we're going to start with a comparison\n",
    "of functions written in and interpreted by Python with the use of compiled functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f869ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std(x):\n",
    "    \"\"\"Compute sample statistics with a for-loop.\n",
    "\n",
    "    Args:\n",
    "      x: One-dimensional array of numbers.\n",
    "\n",
    "    Returns:\n",
    "      A 2-tuple with the mean and standard deviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize sum (s) and sum-of-squares (ss)\n",
    "    s = 0\n",
    "    ss = 0\n",
    "    # calculate s and ss by iterating over x\n",
    "    for i in x:\n",
    "        s += i\n",
    "        ss += i**2\n",
    "    # mean and std. dev. calculations\n",
    "    n = x.size\n",
    "    mean = s / n\n",
    "    variance = (ss / n - mean ** 2) * n / (n - 1)\n",
    "    \n",
    "    return mean, variance ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4df71",
   "metadata": {},
   "source": [
    "Confirm the function is working; it should return approximations to\n",
    "the mean and standard deviation parameters of a sample from a normal\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = random.normal(1, 2, size=100)\n",
    "mean_and_std(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4e21f",
   "metadata": {},
   "source": [
    "The approximation isn't very good for a small sample! We are motivated\n",
    "to use a very big array, say $10^{4}$ numbers, and will compare performance\n",
    "using different tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = random.normal(1, 2, size=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "mean_and_std(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a67882",
   "metadata": {},
   "source": [
    "On this system, the baseline implementation takes between 2 and 3 milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c805d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_mean_and_std = njit(mean_and_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_mean_and_std(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdfb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "compiled_mean_and_std(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "array.mean(), array.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d1dab",
   "metadata": {},
   "source": [
    "lessons learned\n",
    "- numpy is fast because it uses efficient, compiled code to do array operations\n",
    "- sure, you might be able to beat numpy with numba ... was it worth the coding time, and can you write a numerically stable algorithm (the one above is not).\n",
    "- numba is not going to help us with larger-than-memory computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15800ff1",
   "metadata": {},
   "source": [
    "## x. Task Graph\n",
    "\n",
    "A task graph is a collection of functions (nodes) linked through input and output data (edges)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c37390",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "A(random.normal) -->|array| B(mean_and_std)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2801707",
   "metadata": {},
   "source": [
    "The output of the `random.normal` function becomes the input to the `mean_and_std` function.\n",
    "\n",
    "When we think about performance, we have to consider\n",
    "1. the amount of data\n",
    "1. the resources available (typically memory and processing cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840afbd",
   "metadata": {},
   "source": [
    "We usually think about the amount of data in two categories, \"small\" means we can fit all the data in memory on the current system. \"Big\" means we cannot. Obviously this depends on the system in use, so you can't consider these two things separately!\n",
    "\n",
    "In this case, the amount of data is less than the available memory, so it's \"small\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{array.nbytes / 2**20} MiB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751491ce",
   "metadata": {},
   "source": [
    "The other resource we have to consider is how many calculations we can do concurrently, i.e. at the same time.\n",
    "\n",
    "Actually, this is all so interrelated, it's hard to describe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db0c60",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "%%{ init: { 'flowchart': { 'curve': 'linear' } } }%%\n",
    "\n",
    "flowchart LR\n",
    "\n",
    "A(random.normal) -->|array| B(split)\n",
    "B -->|array_0| C0(apply-mean_and_std)\n",
    "B -->|array_1| C1(apply-mean_and_std)\n",
    "B -->|array_2| C2(apply-mean_and_std)\n",
    "subgraph SCHEDULER\n",
    "C0\n",
    "C1\n",
    "C2\n",
    "end\n",
    "C0 ---|result_0| X[ ]:::hide\n",
    "C1 ---|result_1| X\n",
    "C2 ---|result_2| X\n",
    "X --> D(combine-mean_and_std)\n",
    "\n",
    "classDef hide width:0px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b3b89",
   "metadata": {},
   "source": [
    "The split-apply-combine framework is everywhere in data processing; usually used for some form of group-wise calculation. Same idea here, but the split is just on slices and the apply and combine steps have to be capable of calculating results on a slice that can be combined to equal the result you would have gotten on the full array.\n",
    "\n",
    "If a computation can be put into a task graph with `spit`, `apply` and `combine`, then we can process \"big\" data using concurrency.\n",
    "\n",
    "If you start trying to logic through the trade-offs though, why would you do big data concurrently. That implies chopping up your big data into chunks small enough to fit in memory ... and then you can only do one chunk at a time.\n",
    "\n",
    "That's correct! But what if you had access to a distributed system?\n",
    "\n",
    "Or what if there is latency in getting the data? Ugh, I have to think more about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = random.normal(1, 2, size=2**27)\n",
    "print(f\"{array.nbytes / 2**30} GiB\")\n",
    "del array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c893c",
   "metadata": {},
   "source": [
    "Calculate the mean of a 4 GiB array, using 4 splits of 1 GiB arrays. Simultaneously calculating\n",
    "the standard deviation is left as an exercise for the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2eebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "n = 4\n",
    "s = 0\n",
    "for _ in range(n):\n",
    "    array = random.normal(1, 2, size=2**27)\n",
    "    s += array.mean()\n",
    "    del array\n",
    "mean = s / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False, memory_limit=\"1 GiB\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_random = da.random.default_rng(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array = dask_random.normal(1, 2, size=2**29, chunks=\"16 MiB\")\n",
    "dask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b800f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2198d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "mean = dask_array.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1165a7",
   "metadata": {},
   "source": [
    "We just demonstrated two ways of doing larger-than-memory calculations.\n",
    "\n",
    "Our synchronous implemenation (using a for loop) took the strategy of maximizing the use of available memory while processing one chunk: so we used 1 GiB chunks, requiring 4 chunks to get to a 4 GiB array.\n",
    "\n",
    "Our concurrent implementation (using `dask.array`), took the strategy of maximizing the use of available processors: so we used small chunks of 16 MiB, requiring 256 chunks to get to a 4 GiB array.\n",
    "\n",
    "The concurrent implementation was about twice as fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c35283",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cbd5f",
   "metadata": {},
   "source": [
    "## x. Parallel Processing\n",
    "\n",
    "Enough hokey examples, lets process some data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913b7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c48bcf9a",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1bc7e",
   "metadata": {},
   "source": [
    "### ocssw tools projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import earthaccess\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "??common.write_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.setdefault(\"OCSSWROOT\", \"/Users/icarroll/Applications/SeaDAS/ocssw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0beafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tspan = (\"2024-07-01\", \"2024-07-31\")\n",
    "bbox = (-76.75, 36.97, -75.74, 39.01)\n",
    "clouds = (0, 50)\n",
    "results = earthaccess.search_data(\n",
    "    short_name=\"PACE_OCI_L2_BGC_NRT\",\n",
    "    temporal=tspan,\n",
    "    bounding_box=bbox,\n",
    "    cloud_cover=clouds,\n",
    ")\n",
    "paths = earthaccess.download(results, local_path=\"L2_BGC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f304de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = \"L2_BGC/PACE_OCI.20240715T174440.L2.OC_BGC.V2_0.NRT.nc\"\n",
    "ofile = ifile.replace(\"L2\", \"L3b\")\n",
    "par = {\n",
    "    \"ifile\": ifile,\n",
    "    \"ofile\": ofile,\n",
    "    \"l3bprod\": \"chlor_a\",\n",
    "    \"prodtype\": \"regional\",\n",
    "    \"resolution\": \"QD\",\n",
    "}\n",
    "common.write_par(\"l2bin-chlor_a.par\", par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $OCSSWROOT/OCSSW_bash.env\n",
    "\n",
    "l2bin par=l2bin-chlor_a.par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = ofile\n",
    "ofile = ifile.replace(\"L3b\", \"L3m\")\n",
    "par = {\n",
    "    \"ifile\": ifile,\n",
    "    \"ofile\": ofile,\n",
    "}\n",
    "common.write_par(\"l3mapgen-chlor_a.par\", par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source $OCSSWROOT/OCSSW_bash.env\n",
    "\n",
    "l2bin par=l2bin-chlor_a.par par=c-chlor_a.par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cebbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ofile = \"granules/PACE_OCI.L3B.nc\"\n",
    "par = {\n",
    "    \"ifile\": \"l2bin_ifile.txt\",\n",
    "    \"ofile\": ofile,\n",
    "    \"prodtype\": \"regional\",\n",
    "    \"resolution\": 9,\n",
    "    \"flaguse\": \"NONE\",\n",
    "    \"rowgroup\": 2000,\n",
    "}\n",
    "write_par(\"l2bin.par\", par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93635a67",
   "metadata": {},
   "source": [
    "gdalwarp using control point array, osgeo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "from cartopy import crs\n",
    "\n",
    "import geoviews as gv\n",
    "gv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe991f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.DataArray(\n",
    "    data=np.linspace(0, 1, 90*180).reshape((90, 180)),\n",
    "    coords={\"lat\": np.arange(90), \"lon\": np.arange(180)},\n",
    "    name=\"demo\",\n",
    ")\n",
    "ds = ds.rio.write_crs(4326)\n",
    "ds = ds.rio.set_spatial_dims(\"lon\", \"lat\")\n",
    "ds = ds.rio.write_coordinate_system()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d65cde",
   "metadata": {},
   "source": [
    "ah, so the geoloc arrays for rasterio is unreleased, great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06778bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.WarpOptions(geoloc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a27b88",
   "metadata": {},
   "source": [
    "but going all the way to gdal seems really hard. also, seems like it wants something on disk, so that's going to make dask hard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140b261",
   "metadata": {},
   "source": [
    "just try to use KDTree on my own? maybe go back to satPy? idea\n",
    "there was to do resampling on viirs_sdr, then replicate without satpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc88dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = gv.Dataset(ds, crs=ds_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gv.Image(gds, [\"lon\", \"lat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19396958",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = (\n",
    "    ds\n",
    "    .rio.write_crs(ds_crs.to_wkt())\n",
    "    .rio.set_spatial_dims(\"lon\", \"lat\")\n",
    "    .rio.write_coordinate_system()\n",
    ")\n",
    "rds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"RGB.nc\", decode_coords=\"all\").load()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf59376",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = xr.open_dataset(\"RGB.byte.tif\", engine=\"rasterio\").load()\n",
    "rds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c37eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c89f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e453427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[:10] # DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = earthaccess.open(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a86e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.open_dataset(paths[0])\n",
    "obs = xr.open_dataset(paths[0], group=\"geophysical_data\")\n",
    "sen = xr.open_dataset(paths[0], group=\"sensor_band_parameters\")\n",
    "geo = xr.open_dataset(paths[0], group=\"navigation_data\").set_coords((\"longitude\", \"latitude\"))\n",
    "dataset = xr.merge((dataset, obs, sen.coords, geo.coords))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3384430",
   "metadata": {},
   "source": [
    "To get a regular grid at \"full resolution\", we have to resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16faaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat = (\n",
    "    dataset[[\"latitude\", \"longitude\"]]\n",
    "    .reset_coords()\n",
    "    .to_dataarray(\"coordinate\")\n",
    "    .stack({\"pixel\": [\"number_of_lines\", \"pixels_per_line\"]}, create_index=False)\n",
    "    .transpose(\"pixel\", ...)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e79538",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = KDTree(lonlat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af18abc",
   "metadata": {},
   "source": [
    "need a grid\n",
    "need to get the lat lon of a point in the grid\n",
    "use the index on those lat lon points\n",
    "... good lord this is tricky. and now there is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.warp import reproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06627020",
   "metadata": {},
   "outputs": [],
   "source": [
    "reproject(\n",
    "    src,\n",
    "    dst=None,\n",
    "    src_geoloc_array, # the x and y arrays\n",
    "    \n",
    "    src_crs={\"init\": \"EPSG:4326\"}, # Geographic CRS\n",
    "    dst_crs={\"init\": \"EPSG:3857\"}, # Projected CRS\n",
    ")\n",
    "# but how do I use this on 180 bands? only need to do the lookups once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d92800",
   "metadata": {},
   "source": [
    "[back to top](#contents)\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "You have completed the notebook on downloading and opening datasets. We now suggest starting the notebook on ...\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all,scrolled,tags",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
